{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "3QJqt5B_93jS",
        "outputId": "b82af50f-730e-4ea3-b90c-ce53fb01f012"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-753e1c8dfbc8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../../data/processed/ohe_25K_tracks_features_and_labels_all.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtracks_features_and_labels_all_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtracks_features_and_labels_all_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/processed/ohe_25K_tracks_features_and_labels_all.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '../../../data/processed/ohe_25K_tracks_features_and_labels_all.csv'\n",
        "\n",
        "tracks_features_and_labels_all_df = pd.read_csv(file_path)\n",
        "tracks_features_and_labels_all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp0082IfDbzB"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# Set the random seed at the beginning of your code\n",
        "random.seed(42)\n",
        "\n",
        "import warnings\n",
        "# Filter out all FutureWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8Zs6pD4Ejau"
      },
      "outputs": [],
      "source": [
        "grouped_df = tracks_features_and_labels_all_df.groupby(\"track_id\").mean()\n",
        "grouped_df\n",
        "grouped_df_88 = grouped_df.drop('Unnamed: 0',axis=1)\n",
        "grouped_df_88"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5kvNKLTBCQw"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "def get_normalize():\n",
        "    np.random.seed(42)\n",
        "    result = None\n",
        "    result = StandardScaler().fit_transform(grouped_df_88)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF0cPD7FGOz7"
      },
      "outputs": [],
      "source": [
        "feature_name = grouped_df_88.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeNlzp5-e8lX"
      },
      "outputs": [],
      "source": [
        "feature_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmUK2aEMmlye"
      },
      "outputs": [],
      "source": [
        "normalize_df = pd.DataFrame(get_normalize(), columns=feature_name)\n",
        "correlation_matrix = normalize_df.corr()\n",
        "correlation_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh9u3gJ2SAwV"
      },
      "outputs": [],
      "source": [
        "# Set display options for all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Set display options for all rows\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Set precision for decimal points\n",
        "pd.set_option('display.precision', 2)\n",
        "\n",
        "# # Print the complete correlation matrix\n",
        "# display(correlation_matrix)\n",
        "# Select rows where correlation is greater than 0.05 or less than -0.05\n",
        "filtered_rows = correlation_matrix[((correlation_matrix >= 0.6) | (correlation_matrix <= -0.6)) & (abs(correlation_matrix) != 1.0)]\n",
        "# Print the filtered correlation matrix\n",
        "total_values = len(filtered_rows.index) * len(filtered_rows.columns)\n",
        "count = 0\n",
        "\n",
        "strong_correlations = []\n",
        "\n",
        "for x_name in filtered_rows.index:\n",
        "    for y_name in filtered_rows.columns:\n",
        "        value = correlation_matrix.loc[x_name, y_name]\n",
        "        if (value >= 0.83 or value <= -0.83) and abs(value) != 1.0:\n",
        "            strong_correlations.append((x_name, y_name, value))\n",
        "            count += 1\n",
        "\n",
        "# Sort the list of strong correlations based on the absolute correlation values\n",
        "strong_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "\n",
        "# Print the top 10 strongest correlated feature pairs\n",
        "print(\"Top 10 Strongest Correlated Feature Pairs:\")\n",
        "for i, (x_name, y_name, value) in enumerate(strong_correlations[:10], 1):\n",
        "    print(f\"{i}. Feature X: {x_name}, Feature Y: {y_name}, Value: {value:.3f}%\")\n",
        "\n",
        "# Calculate and print the total number of strong correlation values and the percentage\n",
        "percentage = (count / total_values) * 100\n",
        "print(f\"\\nTotal number of strong correlation values: {count} (0.83-1.0: Strong correlation)\")\n",
        "print(f\"Percentage of Total values: {percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKJl78g4m9D5"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming correlation_matrix1 and correlation_matrix2 are your correlation matrices\n",
        "axes = sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\")\n",
        "axes.set_title('Correlation Heatmap with 88 features')\n",
        "\n",
        "# Adjust layout to prevent overlapping titles\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsYaqs14iU6i"
      },
      "source": [
        "**Finding 1:**\n",
        "\n",
        "Deep blue color: Indicates strong negative correlation, meaning an increase in one feature is accompanied by a decrease in another feature.Deep red color: Represents strong positive correlation, implying that an increase in one feature is associated with an increase in another feature.Light color (close to white): Indicates correlation close to zero, suggesting a relatively weak linear relationship between two features.\n",
        "Low correlation: Mainly light color indicates a relatively weak linear relationship between features. In other words, changes in one music feature are not likely to significantly predict changes in another feature. It can be used to avoid features affected by multicollinearity.\n",
        "Nonlinear relationships: The correlation heatmap primarily displays linear relationships. If there are nonlinear relationships between features, the correlation heatmap might not capture these relationships effectively.\n",
        "The top 5 pairs of features with the strongest linear correlations.\n",
        "1. Feature X: spectral_centroid_mean, Feature Y: spectral_rolloff_mean, Value: 0.967%\n",
        "2. Feature X: spectral_centroid_mean, Feature Y: MFCC_2_mean, Value: -0.900%\n",
        "3. Feature X: spectral_rolloff_mean, Feature Y: MFCC_2_mean, Value: -0.894%\n",
        "4. Feature X: spectral_centroid_var, Feature Y: spectral_rolloff_var, Value: 0.867%\n",
        "5. Feature X: MFCC_16_var, Feature Y: MFCC_17_var, Value: 0.834%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppq37Fw-nCMF"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn==0.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RpkyMVrpHgd"
      },
      "outputs": [],
      "source": [
        "!pip install pynndescent==0.5.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljGPEJIonLZz"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a UMAP model with 2 dimensions\n",
        "umap_model = umap.UMAP(n_components=2, n_jobs=2, random_state=42)\n",
        "\n",
        "# Fit UMAP without tqdm progress bar\n",
        "umap_result = umap_model.fit_transform(get_normalize())\n",
        "\n",
        "# Create a 2D scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(umap_result[:, 0], umap_result[:, 1], marker='o', s=1, cmap='viridis')\n",
        "\n",
        "# Add labels, titles, etc.\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.title('UMAP Visualization of 88 Features')\n",
        "\n",
        "# Show the 2D plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q-mMa6UTj4w"
      },
      "outputs": [],
      "source": [
        "from pycaret.clustering import *\n",
        "\n",
        "# Initialize clustering environment\n",
        "exp1 = setup(umap_result,session_id=123)\n",
        "\n",
        "# Create multiple clustering models\n",
        "models = ['kmeans', 'dbscan','hclust', 'birch']\n",
        "\n",
        "# Evaluate models and visualize clustering results\n",
        "for model in models:\n",
        "    # Create the clustering model\n",
        "    print(\"\\033[94m\"+\"\\033[1m\"+model)\n",
        "    clustering_model = create_model(model,num_clusters=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HBxRlaJjowQ"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn==0.24.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTb9enhDfqE_"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "# Define a list of candidate cluster values\n",
        "k_values = [2,3,4,5,6,7,8,9, 10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "# Initialize variables for tracking the best clustering\n",
        "best_calinski_score = -1\n",
        "best_davies_bouldin_score = float(\"inf\")\n",
        "best_k = None\n",
        "\n",
        "# Lists to store Calinski-Harabasz and Davies-Bouldin scores for each k\n",
        "calinski_scores_list = []\n",
        "davies_bouldin_scores_list = []\n",
        "\n",
        "# Iterate over different cluster values\n",
        "for k in k_values:\n",
        "    # Set a random seed for reproducibility\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Create a KMeans model with 'k' clusters\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "\n",
        "    # Fit the model and predict cluster labels on UMAP-reduced data\n",
        "    kmeans_labels = kmeans.fit_predict(umap_result)\n",
        "\n",
        "    # Calculate Calinski-Harabasz score\n",
        "    calinski_score = calinski_harabasz_score(umap_result, kmeans_labels)\n",
        "    calinski_scores_list.append(calinski_score)\n",
        "\n",
        "    # Calculate Davies-Bouldin score\n",
        "    davies_bouldin = davies_bouldin_score(umap_result, kmeans_labels)\n",
        "    davies_bouldin_scores_list.append(davies_bouldin)\n",
        "\n",
        "    # Update the best clustering if a better combination is found\n",
        "    if calinski_score > best_calinski_score and davies_bouldin < best_davies_bouldin_score:\n",
        "        best_calinski_score = calinski_score\n",
        "        best_davies_bouldin_score = davies_bouldin\n",
        "        best_k = k\n",
        "\n",
        "# Print the results of the best clustering\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+f\"Best number of clusters: {best_k}\")\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+f\"Best Calinski-Harabasz score: {best_calinski_score}\")\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+f\"Best Davies-Bouldin score: {best_davies_bouldin_score}\")\n",
        "\n",
        "# Create a figure for plotting\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# First subplot: Calinski-Harabasz Score\n",
        "plt.subplot(2, 1, 1)  # 2 rows, 1 column, subplot 1\n",
        "plt.plot(k_values, calinski_scores_list, marker='o', color='blue')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Calinski-Harabasz Score')\n",
        "plt.title('Calinski-Harabasz Scores vs. Number of UMAP-Reduced Data Kmeans-Clusters with 88 features')\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "\n",
        "# Second subplot: Davies-Bouldin Score\n",
        "plt.subplot(2, 1, 2)  # 2 rows, 1 column, subplot 2\n",
        "plt.plot(k_values, davies_bouldin_scores_list, marker='o', color='red')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Davies-Bouldin Score')\n",
        "plt.title('Davies-Bouldin Score vs. Number of UMAP-Reduced Data Kmeans-Clusters with 88 features')\n",
        "plt.grid(True)\n",
        "\n",
        "# Set x-axis tick positions to the values in k_values\n",
        "plt.xticks(k_values)\n",
        "\n",
        "# Adjust layout to prevent subplot overlap\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7a3As_xE13o"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "eps_values = [0.1, 0.2, 0.3, 0.4, 0.5,0.6,0.7,0.8,0.9]\n",
        "min_samples_values = [5,10, 15, 20]\n",
        "\n",
        "best_calinski_score = -1\n",
        "best_davies_bouldin_score = float(\"inf\")\n",
        "best_eps = None\n",
        "best_min_samples = None\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        dbscan_labels = dbscan.fit_predict(umap_result)\n",
        "\n",
        "        calinski = calinski_harabasz_score(umap_result, dbscan_labels)\n",
        "        davies_bouldin = davies_bouldin_score(umap_result, dbscan_labels)\n",
        "\n",
        "        if calinski > best_calinski_score and davies_bouldin < best_davies_bouldin_score:\n",
        "\n",
        "            best_calinski_score = calinski\n",
        "            best_davies_bouldin_score = davies_bouldin\n",
        "            best_eps = eps\n",
        "            best_min_samples = min_samples\n",
        "\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"Best eps:\", best_eps)\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"Best min_samples:\", best_min_samples)\n",
        "\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"Best Calinski-Harabasz Score:\", best_calinski_score)\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"Best Davies-Bouldin Score:\", best_davies_bouldin_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRAb2VK3FMXm"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dbscan = DBSCAN(eps=best_eps, min_samples= best_min_samples)\n",
        "\n",
        "dbscan_labels = dbscan.fit_predict(umap_result)\n",
        "\n",
        "# Calculate the number of clusters (excluding noise points)\n",
        "num_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"Number of cluster is:\",num_clusters)\n",
        "# Set colors for each cluster, including black for noise points\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, num_clusters + 1))\n",
        "colors = np.vstack((colors, [0, 0, 0, 1]))  # Add black color for noise points\n",
        "\n",
        "# Plot the scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(umap_result[:, 0], umap_result[:, 1], c=dbscan_labels, cmap='viridis', s=1)\n",
        "\n",
        "# Add legend outside the plot\n",
        "legend_labels = list(range(num_clusters)) + ['Noise']\n",
        "legend_labels = [str(label) for label in legend_labels]\n",
        "plt.legend(handles=scatter.legend_elements()[0], title='Clusters', labels=legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Add axis labels and title\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.title('DBSCAN Clustering of UMAP-Reduced Data with 88 features')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1272FPmLZwn"
      },
      "source": [
        "Secondly, Apply PCA to deduce the dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhvZwPvvLcnS"
      },
      "outputs": [],
      "source": [
        "#it typically used to help determine how many principal components (dimensions) to retain in Principal Component Analysis (PCA).\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "pca=None\n",
        "pca = PCA(random_state=42).fit(get_normalize())\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance_ratio = explained_variance_ratio.cumsum()\n",
        "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Variance Explained')\n",
        "plt.title('Cumulative Variance Explained by Principal Components with 88 features')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnmE3N06LvIS"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "cumulative_variance_ratio = explained_variance_ratio.cumsum()\n",
        "desired_variance_ratio = 0.8  #  set 80% as cumulative variance explained threshold\n",
        "components_needed = np.where(cumulative_variance_ratio >= desired_variance_ratio)[0][0] + 1\n",
        "components_needed\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"The components we need in PCA with 88 features:\", components_needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzhSDs1YMQEy"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "np.random.seed(42)\n",
        "pca = PCA(n_components=28,random_state=42)\n",
        "pca_result = pca.fit_transform(get_normalize())\n",
        "\n",
        "# get contribution from each PC\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "eigenvalues = pca.explained_variance_\n",
        "\n",
        "# Calculate the variance of Principal Component 1 (PC1)\n",
        "variance_pc1 = eigenvalues[0]\n",
        "\n",
        "# Calculate the variance of Principal Component 2 (PC2)\n",
        "variance_pc2 = eigenvalues[1]\n",
        "\n",
        "# Calculate the variance of Principal Component 3 (PC3)\n",
        "variance_pc3 = eigenvalues[2]\n",
        "\n",
        "# Calculate the variance contribution ratio\n",
        "total_variance = sum(eigenvalues)\n",
        "variance_ratio_pc1 = variance_pc1 / total_variance\n",
        "variance_ratio_pc2 = variance_pc2 / total_variance\n",
        "variance_ratio_pc3 = variance_pc3 / total_variance\n",
        "\n",
        "# Format the values with two decimal places\n",
        "variance_pc1_formatted = \"{:.2f}\".format(variance_pc1)\n",
        "variance_pc2_formatted = \"{:.2f}\".format(variance_pc2)\n",
        "variance_pc3_formatted = \"{:.2f}\".format(variance_pc3)\n",
        "variance_ratio_pc1_formatted = \"{:.2f}\".format(variance_ratio_pc1)\n",
        "variance_ratio_pc2_formatted = \"{:.2f}\".format(variance_ratio_pc2)\n",
        "variance_ratio_pc3_formatted = \"{:.2f}\".format(variance_ratio_pc3)\n",
        "\n",
        "\n",
        "print(\"Variance of PC1:\", variance_pc1_formatted)\n",
        "print(\"Variance of PC2:\", variance_pc2_formatted)\n",
        "print(\"Variance of PC3:\", variance_pc3_formatted)\n",
        "print(\"Variance Ratio of PC1:\", variance_ratio_pc1_formatted)\n",
        "print(\"Variance Ratio of PC2:\", variance_ratio_pc2_formatted)\n",
        "print(\"Variance Ratio of PC3:\", variance_ratio_pc3_formatted)\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"Variance Ratio of PC1,PC2,PC3 with 88 features:\", float(variance_ratio_pc1_formatted)+float(variance_ratio_pc2_formatted)+float(variance_ratio_pc3_formatted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_G60ezyMtAE"
      },
      "source": [
        "To run KMeans cluster on PCA Reduced Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MZK9BTAM5W6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import umap\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "pca = PCA(n_components=components_needed,random_state=42)\n",
        "pca_result = pca.fit_transform(get_normalize())\n",
        "\n",
        "k_values = [5, 6,7,8,9,10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "best_calinski_score = -1\n",
        "best_davies_bouldin_score = float(\"inf\")\n",
        "\n",
        "best_k = None\n",
        "calinski_scores_list = []\n",
        "davies_bouldin_scores_list = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans_labels = kmeans.fit_predict(pca_result)\n",
        "\n",
        "    calinski_score = calinski_harabasz_score(pca_result, kmeans_labels)\n",
        "    calinski_scores_list.append(calinski_score)\n",
        "\n",
        "    davies_bouldin = davies_bouldin_score(pca_result, kmeans_labels)\n",
        "    davies_bouldin_scores_list.append(davies_bouldin)\n",
        "\n",
        "    if calinski_score > best_calinski_score and davies_bouldin < best_davies_bouldin_score:\n",
        "        best_calinski_score = calinski_score\n",
        "        best_davies_bouldin_score = davies_bouldin\n",
        "        best_k = k\n",
        "\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+f\"Best number of clusters: {best_k}\")\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+f\"Best Calinski-Harabasz score: {best_calinski_score}\")\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+f\"Best Davies-Bouldin score: {best_davies_bouldin_score}\")\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# subplot1：Calinski-Harabasz Score\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(k_values, calinski_scores_list, marker='o', color='blue')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Calinski-Harabasz Score')\n",
        "plt.title('Calinski-Harabasz Scores vs. Number of PCA-Reduced Data Kmeans-Clusters with artist name')\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "\n",
        "# subplot2：Davies-Bouldin Score\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(k_values, davies_bouldin_scores_list, marker='o', color='red')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Davies-Bouldin Score')\n",
        "plt.title('Davies-Bouldin Score vs. Number of PCA-Reduced Data Kmeans-Clusters with artist name')\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol3JhyF0Ooey"
      },
      "source": [
        "**Finding 2:**\n",
        "\n",
        "- This may indicate that the dataset contains complex nonlinear relationships. UMAP is more proficient at capturing these nonlinear relationships during dimensionality reduction, making it more effective to use linear clustering methods on nonlinear data.PCA is primarily designed to capture linear relationships, and as a result, it may perform poorly in clustering tasks with complex nonlinear structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IibhULQO5NW"
      },
      "source": [
        "Create a UMAP model base on PCA reduced data with 2 dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgfGQI5qOyw9"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "umap_model = umap.UMAP(n_components=2, n_jobs=2, random_state=42)\n",
        "\n",
        "# Use fit_transform to reduce the dimensionality of data in key_feature_df to 2 dimensions\n",
        "umap_pca_result = umap_model.fit_transform(pca_result)\n",
        "\n",
        "# Create a 2D scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(umap_pca_result[:, 0],umap_pca_result[:, 1], marker='o', s=1, cmap='viridis')\n",
        "\n",
        "# Add labels, titles, etc.\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.title('UMAP Visualization of Key Features with 88 features')\n",
        "\n",
        "# Show the 2D plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ57Z0tpPfYV"
      },
      "source": [
        "KMeans model based on UMAP and PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HaUa1ziPq6g"
      },
      "outputs": [],
      "source": [
        "k_values = [2,3,4,5,6,7,8,9,10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "best_calinski_score = -1\n",
        "best_davies_bouldin_score = float(\"inf\")\n",
        "\n",
        "best_k = None\n",
        "calinski_scores_list = []\n",
        "davies_bouldin_scores_list = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans_labels = kmeans.fit_predict(umap_pca_result)\n",
        "\n",
        "    calinski_score = calinski_harabasz_score(umap_pca_result, kmeans_labels)\n",
        "    calinski_scores_list.append(calinski_score)\n",
        "\n",
        "    davies_bouldin = davies_bouldin_score(umap_pca_result, kmeans_labels)\n",
        "    davies_bouldin_scores_list.append(davies_bouldin)\n",
        "\n",
        "    if calinski_score > best_calinski_score and davies_bouldin < best_davies_bouldin_score:\n",
        "        best_calinski_score = calinski_score\n",
        "        best_davies_bouldin_score = davies_bouldin\n",
        "        best_k = k\n",
        "\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+f\"Best number of clusters: {best_k}\")\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+f\"Best Calinski-Harabasz score: {best_calinski_score}\")\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+f\"Best Davies-Bouldin score: {best_davies_bouldin_score}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Calinski-Harabasz Score\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(k_values, calinski_scores_list, marker='o', color='blue')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Calinski-Harabasz Score')\n",
        "plt.title('Calinski-Harabasz Scores vs. Number of UMAP-Reduced Data Kmeans-Clusters with 88 features')\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "\n",
        "# Davies-Bouldin Score\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(k_values, davies_bouldin_scores_list, marker='o', color='red')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Davies-Bouldin Score')\n",
        "plt.title('Davies-Bouldin Score vs. Number of UMAP-Reduced Data Kmeans-Clusters with 88 features')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.xticks(k_values)\n",
        "\n",
        "plt.tight_layout()  # ensure subplot not overlap\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4CFNJKlktcR"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "eps_values = [0.1, 0.2, 0.3, 0.4, 0.5,0.6,0.7,0.8,0.9]\n",
        "min_samples_values = [5,10, 15, 20]\n",
        "\n",
        "best_calinski_score = -1\n",
        "best_davies_bouldin_score = float(\"inf\")\n",
        "best_eps = None\n",
        "best_min_samples = None\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        dbscan_labels = dbscan.fit_predict(umap_pca_result)\n",
        "\n",
        "        calinski = calinski_harabasz_score(umap_pca_result, dbscan_labels)\n",
        "        davies_bouldin = davies_bouldin_score(umap_pca_result, dbscan_labels)\n",
        "\n",
        "        if calinski > best_calinski_score and davies_bouldin < best_davies_bouldin_score:\n",
        "\n",
        "            best_calinski_score = calinski\n",
        "            best_davies_bouldin_score = davies_bouldin\n",
        "            best_eps = eps\n",
        "            best_min_samples = min_samples\n",
        "\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"Best eps:\", best_eps)\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"Best min_samples:\", best_min_samples)\n",
        "\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"Best Calinski-Harabasz Score:\", best_calinski_score)\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"Best Davies-Bouldin Score:\", best_davies_bouldin_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wu4jsgtlFEZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dbscan = DBSCAN(eps=0.4, min_samples= 5)\n",
        "\n",
        "dbscan_labels = dbscan.fit_predict(umap_pca_result)\n",
        "\n",
        "# Calculate the number of clusters (excluding noise points)\n",
        "num_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
        "print(\"\\033[94m\"+\"\\033[1m\"+\"Number of cluster is:\",num_clusters)\n",
        "# Set colors for each cluster, including black for noise points\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, num_clusters + 1))\n",
        "colors = np.vstack((colors, [0, 0, 0, 1]))  # Add black color for noise points\n",
        "\n",
        "# Plot the scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(umap_pca_result[:, 0], umap_pca_result[:, 1], c=dbscan_labels, cmap='viridis', s=1)\n",
        "\n",
        "# Add legend outside the plot\n",
        "legend_labels = list(range(num_clusters)) + ['Noise']\n",
        "legend_labels = [str(label) for label in legend_labels]\n",
        "plt.legend(handles=scatter.legend_elements()[0], title='Clusters', labels=legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Add axis labels and title\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.title('DBSCAN Clustering of UMAP-Reduced Data with 88 features')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h6UDvNwQLFV"
      },
      "source": [
        "**Finding 3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- If clustering methods are applied to data that has been jointly processed by UMAP and PCA, regardless of which clustering methodology is used, the metric test scores are significantly better than when using PCA alone to process the data.\n",
        "- The original data is also applicable in cases where PCA and UMAP are jointly applied for processing.\n",
        "- Especially when clustering using the DBscan method, we found that the results were even better than those obtained by processing the data with UMAP alone. Therefore, we ultimately chose the DBscan clustering method to extract cluster labels.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1rUMejB5nlYh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx27TpVgQa4K"
      },
      "outputs": [],
      "source": [
        "# from pycaret.clustering import *\n",
        "\n",
        "# # Initialize clustering environment\n",
        "# exp1 = setup(umap_pca_result,session_id=123)\n",
        "\n",
        "# # Create multiple clustering models\n",
        "# models = ['kmeans', 'dbscan','hclust', 'birch']\n",
        "\n",
        "# # Evaluate models and visualize clustering results\n",
        "# for model in models:\n",
        "#     # Create the clustering model\n",
        "#     print(\"\\033[94m\"+\"\\033[1m\"+model)\n",
        "#     clustering_model = create_model(model,num_clusters=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJGYoFMMkxVS"
      },
      "outputs": [],
      "source": [
        "# kmeans_umap = KMeans(n_clusters=15, random_state=42)\n",
        "# kmeans_umap_labels = kmeans_umap.fit_predict(umap_result)\n",
        "\n",
        "# grouped_umap_15_88feature_df = grouped_df_88.copy()\n",
        "# grouped_umap_15_88feature_df['KMeans_umap_Labels'] = kmeans_umap_labels\n",
        "\n",
        "# grouped_umap_15_88feature_df.to_csv('grouped_umap_15_88feature_df.csv',index=True)\n",
        "# grouped_umap_15_88feature_df['KMeans_umap_Labels'].value_counts()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ydtvOsMsFeN"
      },
      "outputs": [],
      "source": [
        "dbscan = DBSCAN(eps=0.4, min_samples=5)\n",
        "\n",
        "dbscan_labels = dbscan.fit_predict(umap_pca_result)\n",
        "\n",
        "grouped_dbscan_12_88feature_df = grouped_df_88.copy()\n",
        "grouped_dbscan_12_88feature_df['dbscan_umap_pca_Labels'] = dbscan_labels\n",
        "\n",
        "grouped_dbscan_12_88feature_df.to_csv('grouped_dbscan_12_88feature_df.csv',index=True)\n",
        "grouped_dbscan_12_88feature_df['dbscan_umap_pca_Labels'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eajLZOnkxBc-"
      },
      "outputs": [],
      "source": [
        "grouped_dbscan_12_88feature_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUDeOdDb-wKL"
      },
      "outputs": [],
      "source": [
        "# grouped_umap_15_88feature_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koMIsSgMxLoz"
      },
      "outputs": [],
      "source": [
        "merged_df_dbscan = pd.merge(grouped_dbscan_12_88feature_df, tracks_features_and_labels_all_df[['track_id', 'track_genre_top']], on='track_id', how='left')\n",
        "\n",
        "# Create a new 'target' column and fill it with 'track_genre_top' values\n",
        "merged_df_dbscan['target'] = merged_df_dbscan['track_genre_top']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSgPqeRcxml8"
      },
      "outputs": [],
      "source": [
        "merged_df_dbscan = merged_df_dbscan.drop('track_genre_top', axis=1)\n",
        "\n",
        "merged_df_with_dbscan_umap_Labels = merged_df_dbscan.drop('track_id',axis=1)\n",
        "# merged_df_with_KMeans_umap_Labels_without_keyscale = merged_df.drop(['track_id', 'key_A', 'key_Ab', 'key_B', 'key_Bb',\n",
        "#        'key_C', 'key_C#', 'key_D', 'key_E', 'key_Eb', 'key_F', 'key_F#',\n",
        "#        'key_G', 'scale_major', 'scale_minor'],axis=1)\n",
        "display(merged_df_with_dbscan_umap_Labels)\n",
        "\n",
        "merged_df_no_dbscan_umap_Labels = merged_df_with_dbscan_umap_Labels.drop('dbscan_umap_pca_Labels',axis=1)\n",
        "# merged_df_no_KMeans_umap_Labels_without_keyscale = merged_df.drop(['track_id', 'key_A', 'key_Ab', 'key_B', 'key_Bb',\n",
        "#        'key_C', 'key_C#', 'key_D', 'key_E', 'key_Eb', 'key_F', 'key_F#',\n",
        "#        'key_G', 'scale_major', 'scale_minor','KMeans_umap_Labels'],axis=1)\n",
        "merged_df_no_dbscan_umap_Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ztqpgrefl3gW"
      },
      "outputs": [],
      "source": [
        "# merged_df = pd.merge(grouped_umap_15_88feature_df, tracks_features_and_labels_all_df[['track_id', 'track_genre_top']], on='track_id', how='left')\n",
        "\n",
        "# # Create a new 'target' column and fill it with 'track_genre_top' values\n",
        "# merged_df['target'] = merged_df['track_genre_top']\n",
        "\n",
        "\n",
        "\n",
        "# # Drop the redundant 'track_genre_top' column\n",
        "# merged_df = merged_df.drop('track_genre_top', axis=1)\n",
        "\n",
        "# merged_df_with_KMeans_umap_Labels = merged_df.drop('track_id',axis=1)\n",
        "# merged_df_with_KMeans_umap_Labels_without_keyscale = merged_df.drop(['track_id', 'key_A', 'key_Ab', 'key_B', 'key_Bb',\n",
        "#        'key_C', 'key_C#', 'key_D', 'key_E', 'key_Eb', 'key_F', 'key_F#',\n",
        "#        'key_G', 'scale_major', 'scale_minor'],axis=1)\n",
        "# display(merged_df_with_KMeans_umap_Labels)\n",
        "# merged_df_no_KMeans_umap_Labels = merged_df_with_KMeans_umap_Labels.drop('KMeans_umap_Labels',axis=1)\n",
        "# merged_df_no_KMeans_umap_Labels_without_keyscale = merged_df.drop(['track_id', 'key_A', 'key_Ab', 'key_B', 'key_Bb',\n",
        "#        'key_C', 'key_C#', 'key_D', 'key_E', 'key_Eb', 'key_F', 'key_F#',\n",
        "#        'key_G', 'scale_major', 'scale_minor','KMeans_umap_Labels'],axis=1)\n",
        "# merged_df_no_KMeans_umap_Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wq7zCu2y7BN"
      },
      "outputs": [],
      "source": [
        "merged_df_dbscan.to_csv('merged_one_hot_df_dbscan_88.csv',index=False)\n",
        "merged_df_dbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R95_-uyYPmiK"
      },
      "outputs": [],
      "source": [
        "# merged_df_with_KMeans_umap_Labels.to_csv('/content/drive/My Drive/Colab Notebooks/merged_df_with_KMeans_umap_Labels.csv',index=True)\n",
        "# merged_df_no_KMeans_umap_Labels.to_csv('/content/drive/My Drive/Colab Notebooks/merged_df_no_KMeans_umap_Labels.csv',index=True)\n",
        "merged_df.to_csv('merged_one_hot_df_88.csv',index=False)\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0g97sSLzPxs"
      },
      "outputs": [],
      "source": [
        "from pycaret.classification import *\n",
        "\n",
        "# Initialize clustering environment\n",
        "exp1 = setup(data=merged_df_with_dbscan_umap_Labels,\n",
        "             target=\"target\",\n",
        "             categorical_features=[\"dbscan_umap_pca_Labels\"],\n",
        "             normalize=True,\n",
        "             fix_imbalance=True,\n",
        "             session_id=123)\n",
        "\n",
        "\n",
        "model = create_model('lightgbm',class_weight='balanced')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqQOxVzWzSRb"
      },
      "outputs": [],
      "source": [
        "from pycaret.classification import *\n",
        "\n",
        "\n",
        "# Initialize clustering environment\n",
        "exp1 = setup(data=merged_df_no_dbscan_umap_Labels,\n",
        "             target=\"target\",\n",
        "             normalize=True,\n",
        "             fix_imbalance=True,\n",
        "             session_id=123)\n",
        "\n",
        "model = create_model('lightgbm', class_weight='balanced')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAnmJIkeqA_1"
      },
      "outputs": [],
      "source": [
        "# from pycaret.classification import *\n",
        "\n",
        "# # Initialize clustering environment\n",
        "# exp1 = setup(data=merged_df_with_KMeans_umap_Labels,\n",
        "#              target=\"target\",\n",
        "#              categorical_features=[\"KMeans_umap_Labels\"],\n",
        "#              normalize=True,\n",
        "#              fix_imbalance=True,\n",
        "#              session_id=123)\n",
        "\n",
        "\n",
        "# model = create_model('lightgbm',class_weight='balanced')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS4lNIxzdr7U"
      },
      "outputs": [],
      "source": [
        "# from pycaret.classification import *\n",
        "\n",
        "\n",
        "# # Initialize clustering environment\n",
        "# exp1 = setup(data=merged_df_no_KMeans_umap_Labels,\n",
        "#              target=\"target\",\n",
        "#              normalize=True,\n",
        "#              fix_imbalance=True,\n",
        "#              session_id=123)\n",
        "\n",
        "# model = create_model('lightgbm', class_weight='balanced')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTGM-rbEf6b0"
      },
      "outputs": [],
      "source": [
        "# from pycaret.classification import *\n",
        "\n",
        "\n",
        "# # Initialize clustering environment\n",
        "# exp1 = setup(data=merged_df_no_KMeans_umap_Labels_without_keyscale,\n",
        "#              target=\"target\",\n",
        "#              normalize=True,\n",
        "#              fix_imbalance=True,\n",
        "#              session_id=123)\n",
        "\n",
        "# model = create_model('lightgbm', class_weight='balanced')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsrq71O1fy0X"
      },
      "outputs": [],
      "source": [
        "# from pycaret.classification import *\n",
        "\n",
        "\n",
        "# # Initialize clustering environment\n",
        "# exp1 = setup(data=merged_df_with_KMeans_umap_Labels_without_keyscale,\n",
        "#              target=\"target\",\n",
        "#              normalize=True,\n",
        "#              fix_imbalance=True,\n",
        "#              session_id=123)\n",
        "\n",
        "# model = create_model('lightgbm', class_weight='balanced')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding 4:**\n",
        "\n",
        "- Although the experiments above show that the clustering effect of DBscan is quite good, when we use the obtained cluster labels as additional features and incorporate them into the supervised learning model training with the existing 88 features, we did not observe a significant improvement in the model's performance.\n",
        "- It is possible that the clustering labels are strongly correlated with features such as \"key\" and \"scale.\" Alternatively, errors in the genre types of the original data could be causing the issue.\n",
        "- Therefore, in the end, we decided not to incorporate the clustering labels obtained from unsupervised learning into our dataset for supervised learning training and evaluation."
      ],
      "metadata": {
        "id": "6Q3TtSFFrREe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DpdkXtERsadN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}